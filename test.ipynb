{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#from aux import ensembler, json_to_Dataset_ensemble\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembler(output1, output2, word_ids1, word_ids2):\n",
    "    word_ids1 = word_ids1[1:-1]\n",
    "    word_ids2 = word_ids2[1:-1]\n",
    "    output1 = output1[1:-1]\n",
    "    output2 = output2[1:-1]\n",
    "\n",
    "    stacked_tensors1 = torch.stack([torch.tensor(i) for i in output1])\n",
    "    placeholder1 = torch.mean(stacked_tensors1, dim=0)\n",
    "\n",
    "    stacked_tensors2 = torch.stack([torch.tensor(i) for i in output2])\n",
    "    placeholder2 = torch.mean(stacked_tensors2, dim=0)\n",
    "\n",
    "    new_output1 = []\n",
    "    new_output2 = []\n",
    "\n",
    "    current_word = []\n",
    "    prev_word_id = 0\n",
    "    for ind, word_id in enumerate(word_ids1):\n",
    "        if word_id != prev_word_id:\n",
    "            if word_id > prev_word_id + 1:\n",
    "                new_output1.append(placeholder1)\n",
    "            prev_word_id = word_id\n",
    "            stacked_tensors = torch.stack(current_word)\n",
    "            averaged_tensor = torch.mean(stacked_tensors, dim=0)\n",
    "            new_output1.append(averaged_tensor.tolist())\n",
    "            current_word = []\n",
    "        current_word.append(output1[ind])\n",
    "\n",
    "    current_word = []\n",
    "    prev_word_id = 0\n",
    "    for ind, word_id in enumerate(word_ids2):\n",
    "        if word_id != prev_word_id:\n",
    "            if word_id > prev_word_id + 1:\n",
    "                new_output2.append(placeholder2)\n",
    "            prev_word_id = word_id\n",
    "            stacked_tensors = torch.stack(current_word)\n",
    "            averaged_tensor = torch.mean(stacked_tensors, dim=0)\n",
    "            new_output2.append(averaged_tensor.tolist())\n",
    "            current_word = []\n",
    "        current_word.append(output2[ind])\n",
    "\n",
    "    return torch.tensor(new_output1), torch.tensor(new_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KingBert(nn.Module):\n",
    "    def __init__(self, distilbert_tuned, albert_tuned):\n",
    "        super().__init__()\n",
    "        self.distilbert = distilbert_tuned\n",
    "        self.albert = albert_tuned\n",
    "\n",
    "        for distilbert_param in self.distilbert.parameters():\n",
    "            distilbert_param.requires_grad = False\n",
    "\n",
    "        for albert_param in self.albert.parameters():\n",
    "            albert_param.requires_grad = False \n",
    "        \n",
    "        #Here we have an alpha for each label\n",
    "        self.alpha = nn.Parameter(0.5 * torch.ones(47), requires_grad = True)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, distilbert_input_ids, albert_input_ids, distil_attention_mask, alb_attention_mask, distilbert_word_ids, albert_word_ids):\n",
    "        distilbert_output = self.distilbert(input_ids=torch.tensor([distilbert_input_ids]), attention_mask=torch.tensor([distil_attention_mask]))\n",
    "        albert_output = self.albert(input_ids=torch.tensor([albert_input_ids]), attention_mask=torch.tensor([alb_attention_mask]))\n",
    "        distilbert_fixed, albert_fixed = ensembler(distilbert_output['logits'].squeeze(), albert_output['logits'].squeeze(), distilbert_word_ids, albert_word_ids)\n",
    "\n",
    "        final_output = distilbert_fixed * self.alpha + albert_fixed * (torch.ones(47) - self.alpha)\n",
    "\n",
    "        return self.softmax(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert = AutoModelForTokenClassification.from_pretrained('distilbert_finetuned')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert_finetuned')\n",
    "albert = AutoModelForTokenClassification.from_pretrained('albert_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "kingbert = KingBert(distilbert_tuned=distilbert, albert_tuned=albert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_to_Dataset_ensemble(\"data/ensemble_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['spacy_labels', 'albert_inputids', 'distilbert_inputids', 'albert_wordids', 'distilbert_wordids', 'albert_attention_masks', 'distilbert_attention_masks'],\n",
       "    num_rows: 18244\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([86, 47]) torch.Size([86, 47])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/05/8k53g1bs725dn8cs5310zydm0000gn/T/ipykernel_72585/358254760.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  stacked_tensors1 = torch.stack([torch.tensor(i) for i in output1])\n",
      "/var/folders/05/8k53g1bs725dn8cs5310zydm0000gn/T/ipykernel_72585/358254760.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  stacked_tensors2 = torch.stack([torch.tensor(i) for i in output2])\n"
     ]
    }
   ],
   "source": [
    "res = kingbert(data[0]['distilbert_inputids'], data[0]['albert_inputids'], data[0]['distilbert_attention_masks'], data[0]['albert_attention_masks'], data[0]['distilbert_wordids'], data[0]['albert_wordids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([86, 47])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.argmax(res, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
