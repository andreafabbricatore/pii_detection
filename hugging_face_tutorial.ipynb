{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ai4privacy/pii-masking-300k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
       "        num_rows: 177677\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
       "        num_rows: 47728\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_labels = dataset['train'][:29908]['mbert_text_tokens']\n",
    "target_labels = dataset['train'][:29908]['mbert_bio_labels']\n",
    "document_ids = dataset['train'][:29908]['id']\n",
    "full_texts = dataset['train'][:29908]['source_text']\n",
    "privacy_mask = dataset['train'][:29908]['privacy_mask']\n",
    "\n",
    "train_dataset = [{'id':document_ids[i][:-1], 'ner_tags': target_labels[i], 'tokens': input_labels[i], 'full_text': full_texts[i].replace(\"\\\\n\",\" \"), 'privacy_mask': privacy_mask[i]} for i in range(29908)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = ['B-STREET',\n",
    " 'B-CITY',\n",
    " 'I-DATE',\n",
    " 'B-PASS',\n",
    " 'I-CITY',\n",
    " 'B-TIME',\n",
    " 'B-EMAIL',\n",
    " 'I-DRIVERLICENSE',\n",
    " 'I-POSTCODE',\n",
    " 'I-BOD',\n",
    " 'B-USERNAME',\n",
    " 'B-BOD',\n",
    " 'B-COUNTRY',\n",
    " 'B-SECADDRESS',\n",
    " 'I-GIVENNAME2',\n",
    " 'B-IDCARD',\n",
    " 'I-SOCIALNUMBER',\n",
    " 'I-PASSPORT',\n",
    " 'B-IP',\n",
    " 'O',\n",
    " 'B-LASTNAME2',\n",
    " 'B-TEL',\n",
    " 'B-SOCIALNUMBER',\n",
    " 'I-TIME',\n",
    " 'B-BUILDING',\n",
    " 'B-LASTNAME1',\n",
    " 'B-PASSPORT',\n",
    " 'I-TITLE',\n",
    " 'I-SEX',\n",
    " 'I-STREET',\n",
    " 'B-STATE',\n",
    " 'I-STATE',\n",
    " 'B-TITLE',\n",
    " 'I-GIVENNAME1',\n",
    " 'B-DATE',\n",
    " 'B-GEOCOORD',\n",
    " 'I-IDCARD',\n",
    " 'I-TEL',\n",
    " 'B-GIVENNAME2',\n",
    " 'B-POSTCODE',\n",
    " 'I-LASTNAME2',\n",
    " 'B-DRIVERLICENSE',\n",
    " 'I-LASTNAME3',\n",
    " 'I-GEOCOORD',\n",
    " 'I-COUNTRY',\n",
    " 'I-EMAIL',\n",
    " 'I-PASS',\n",
    " 'B-SEX',\n",
    " 'B-LASTNAME3',\n",
    " 'I-USERNAME',\n",
    " 'I-BUILDING',\n",
    " 'I-IP',\n",
    " 'B-GIVENNAME1',\n",
    " 'I-LASTNAME1',\n",
    " 'I-SECADDRESS',\n",
    " 'B-CARDISSUER',\n",
    " 'I-CARDISSUER']\n",
    "id2label = {i: l for i, l in enumerate(all_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "target = [l for l in all_labels if l != \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"full_text\"], truncation=True, is_split_into_words=False)\n",
    "    print(len(tokenized_inputs['input_ids']))\n",
    "    print(examples['ner_tags'])\n",
    "    print(tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids']))\n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    print(len(word_ids))\n",
    "    print(word_ids)\n",
    "    for i, label in enumerate(word_ids):\n",
    "        word_idx = label\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:  \n",
    "            labels.append(label2id[examples['ner_tags'][label]])\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for i in range(len(l)-sll):\n",
    "        if l[i:i+sll] == sl:\n",
    "            return i, i+sll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sub_list([1,2], [1,1,3,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = []\n",
    "from ast import literal_eval\n",
    "for item in train_dataset: \n",
    "    privacy_mask = item['privacy_mask']\n",
    "    #getting tokenized text\n",
    "    tokenized_inputs = deberta_tokenizer(item[\"full_text\"], truncation=True, is_split_into_words=False)\n",
    "    #print(tokenized_inputs)\n",
    "    #tokenizing mask labels to hopefully replace in tokenized_inputs\n",
    "    tags = ['O' for i in range(len(tokenized_inputs['input_ids']))]\n",
    "    append = True\n",
    "    for mask in privacy_mask:\n",
    "        #print(item['full_text'][mask[0]:mask[1]])\n",
    "        tokenized_mask = deberta_tokenizer(item['full_text'][mask['start']:mask['end']], truncation=True, is_split_into_words=False)\n",
    "        mask_ids = tokenized_mask['input_ids'][1:-1]\n",
    "        #print(mask_ids)\n",
    "        match_indices = find_sub_list(mask_ids, tokenized_inputs['input_ids'])\n",
    "        if match_indices == None:\n",
    "            append = False\n",
    "            # print(item['full_text'][mask['start']:mask['end']])\n",
    "            # print(mask['value'])\n",
    "            # print(deberta_tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids']))\n",
    "            # print(deberta_tokenizer.convert_ids_to_tokens(mask_ids))\n",
    "            break\n",
    "            \n",
    "        #print(tokenized_inputs['input_ids'][match_indices[0]:match_indices[1]])\n",
    "        #print(match_indices)\n",
    "        tags[match_indices[0]] = 'B-' + mask['label']\n",
    "        #print(len(tags[match_indices[0]+1:match_indices[1]]) == len(range(len(mask_ids)-1)))\n",
    "        tags[match_indices[0]+1:match_indices[1]] = ['I-' + mask['label'] for i in range(len(mask_ids)-1)]\n",
    "    if append:\n",
    "        tokenized_train_dataset.append({\n",
    "            'tokenized_input_ids': tokenized_inputs['input_ids'],\n",
    "            'tokenized_labels': [-100] + [label2id[i] for i in tags[1:-1]] + [-100]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 111)\n",
      "(98, 101)\n",
      "(86, 95)\n",
      "(82, 85)\n",
      "(74, 79)\n",
      "(72, 73)\n",
      "(65, 69)\n",
      "(62, 64)\n",
      "(53, 59)\n",
      "(23, 25)\n",
      "(17, 20)\n",
      "(13, 16)\n",
      "(8, 10)\n",
      "(4, 7)\n",
      "(83, 84)\n",
      "None\n",
      "oo.com    Social \n",
      "['[CLS]', '▁Subject', ':', '▁Admission', '▁Notification', '▁-', '▁Great', '▁Britain', '▁University', '▁Dear', '▁Applicants', ',', '▁We', '▁are', '▁thrilled', '▁to', '▁inform', '▁you', '▁about', '▁the', '▁status', '▁of', '▁your', '▁admission', '▁to', '▁Great', '▁Britain', '▁University', '.', '▁Please', '▁read', '▁the', '▁details', '▁below', '▁for', '▁the', '▁automated', '▁notification', '.', '▁Date', '▁of', '▁Notification', ':', '▁5', ':24', 'am', '▁on', '▁August', '▁5', 'th', ',', '▁20', '57', '▁**', 'App', 'lic', 'ant', '▁Details', '**', '▁1.', '▁Applicant', ':', '▁Ball', 'oi', '▁Eck', 'rich', '▁Email', ':', '▁b', 'ball', 'oi', '@', 'yahoo', '.', 'com', '▁Social', '▁Number', ':', '▁996', '▁07', '6', '▁6', '460', '▁ID', '[SEP]']\n",
      "['▁oo', '.', 'com', '▁Social']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(deberta_tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(span_ids))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print(tokenized_inputs['input_ids'][match_indices[0]:match_indices[1]])\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#print(match_indices)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m tags[\u001b[43mmatch_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m span[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#print(len(tags[match_indices[0]+1:match_indices[1]]) == len(range(len(span_ids)-1)))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m tags[match_indices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:match_indices[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m span[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(span_ids)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = []\n",
    "from ast import literal_eval\n",
    "for item in train_dataset: \n",
    "    span_labels = literal_eval(item['span_labels'])\n",
    "    #getting tokenized text\n",
    "    tokenized_inputs = deberta_tokenizer(item[\"full_text\"], truncation=True, is_split_into_words=False)\n",
    "    #print(tokenized_inputs)\n",
    "    #tokenizing span labels to hopefully replace in tokenized_inputs\n",
    "    tags = ['O' for i in range(len(tokenized_inputs['input_ids']))]\n",
    "    for span in span_labels:\n",
    "        #print(item['full_text'][span[0]:span[1]])\n",
    "        tokenized_span = deberta_tokenizer(item['full_text'][span[0]:span[1]], truncation=True, is_split_into_words=False)\n",
    "        span_ids = tokenized_span['input_ids'][1:-1]\n",
    "        #print(span_ids)\n",
    "        match_indices = find_sub_list(span_ids, tokenized_inputs['input_ids'])\n",
    "        print(match_indices)\n",
    "        if match_indices == None:\n",
    "            print(item['full_text'][span[0]:span[1]])\n",
    "            print(deberta_tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids']))\n",
    "            print(deberta_tokenizer.convert_ids_to_tokens(span_ids))\n",
    "            \n",
    "        #print(tokenized_inputs['input_ids'][match_indices[0]:match_indices[1]])\n",
    "        #print(match_indices)\n",
    "        tags[match_indices[0]] = 'B-' + span[2]\n",
    "        #print(len(tags[match_indices[0]+1:match_indices[1]]) == len(range(len(span_ids)-1)))\n",
    "        tags[match_indices[0]+1:match_indices[1]] = ['I-' + span[2] for i in range(len(span_ids)-1)]\n",
    "\n",
    "    tokenized_train_dataset.append({\n",
    "        'tokenized_input_ids': tokenized_inputs['input_ids'],\n",
    "        'tokenized_labels': [label2id[i] for i in tags]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForTokenClassification, TrainingArguments, Trainer\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/deberta-v2-xlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mall_labels\u001b[49m), id2label\u001b[38;5;241m=\u001b[39mid2label, label2id\u001b[38;5;241m=\u001b[39mlabel2id\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v2-xlarge\", num_labels=len(all_labels), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
