{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ai4privacy/pii-masking-300k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
       "        num_rows: 177677\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set'],\n",
       "        num_rows: 47728\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_labels = dataset['train'][:29908]['mbert_text_tokens']\n",
    "target_labels = dataset['train'][:29908]['mbert_bio_labels']\n",
    "document_ids = dataset['train'][:29908]['id']\n",
    "full_texts = dataset['train'][:29908]['source_text']\n",
    "span_labels = dataset['train'][:29908]['span_labels']\n",
    "\n",
    "train_dataset = [{'id':document_ids[i][:-1], 'ner_tags': target_labels[i], 'tokens': input_labels[i], 'full_text': full_texts[i], 'span_labels': span_labels[i]} for i in range(29908)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = ['B-STREET',\n",
    " 'B-CITY',\n",
    " 'I-DATE',\n",
    " 'B-PASS',\n",
    " 'I-CITY',\n",
    " 'B-TIME',\n",
    " 'B-EMAIL',\n",
    " 'I-DRIVERLICENSE',\n",
    " 'I-POSTCODE',\n",
    " 'I-BOD',\n",
    " 'B-USERNAME',\n",
    " 'B-BOD',\n",
    " 'B-COUNTRY',\n",
    " 'B-SECADDRESS',\n",
    " 'I-GIVENNAME2',\n",
    " 'B-IDCARD',\n",
    " 'I-SOCIALNUMBER',\n",
    " 'I-PASSPORT',\n",
    " 'B-IP',\n",
    " 'O',\n",
    " 'B-LASTNAME2',\n",
    " 'B-TEL',\n",
    " 'B-SOCIALNUMBER',\n",
    " 'I-TIME',\n",
    " 'B-BUILDING',\n",
    " 'B-LASTNAME1',\n",
    " 'B-PASSPORT',\n",
    " 'I-TITLE',\n",
    " 'I-SEX',\n",
    " 'I-STREET',\n",
    " 'B-STATE',\n",
    " 'I-STATE',\n",
    " 'B-TITLE',\n",
    " 'I-GIVENNAME1',\n",
    " 'B-DATE',\n",
    " 'B-GEOCOORD',\n",
    " 'I-IDCARD',\n",
    " 'I-TEL',\n",
    " 'B-GIVENNAME2',\n",
    " 'B-POSTCODE',\n",
    " 'I-LASTNAME2',\n",
    " 'B-DRIVERLICENSE',\n",
    " 'I-LASTNAME3',\n",
    " 'I-GEOCOORD',\n",
    " 'I-COUNTRY',\n",
    " 'I-EMAIL',\n",
    " 'I-PASS',\n",
    " 'B-SEX',\n",
    " 'B-LASTNAME3',\n",
    " 'I-USERNAME',\n",
    " 'I-BUILDING',\n",
    " 'I-IP',\n",
    " 'B-GIVENNAME1',\n",
    " 'I-LASTNAME1',\n",
    " 'I-SECADDRESS',\n",
    " 'B-CARDISSUER',\n",
    " 'I-CARDISSUER']\n",
    "id2label = {i: l for i, l in enumerate(all_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "target = [l for l in all_labels if l != \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      3\u001b[0m deberta_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/deberta-v2-xlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m mbert_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-bert/bert-base-multilingual-cased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "deberta_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert True == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"full_text\"], truncation=True, is_split_into_words=False)\n",
    "    print(len(tokenized_inputs['input_ids']))\n",
    "    print(examples['ner_tags'])\n",
    "    print(tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids']))\n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    print(len(word_ids))\n",
    "    print(word_ids)\n",
    "    for i, label in enumerate(word_ids):\n",
    "        word_idx = label\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:  \n",
    "            labels.append(label2id[examples['ner_tags'][label]])\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_labels(inputstring:str, mapping:list):\n",
    "    for item in mapping:\n",
    "        inputstring = inputstring[:item[0]] + item[2] + inputstring[item[1]:]\n",
    "    return inputstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Group Messaging for Admissions Process\n",
      "\n",
      "Good morning, everyone,\n",
      "\n",
      "I hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developments and key information. Please find below the timeline for our upcoming meetings:\n",
      "\n",
      "- wynqvrh053 - Meeting at 10:20am\n",
      "- luka.burg - Meeting at 21\n",
      "- qahil.wittauer - Meeting at quarter past 13\n",
      "- gholamhossein.ruschke - Meeting at 9:47 PM\n",
      "- pdmjrsyoz1460 \n",
      "Subject: Group Messaging for Admissions Process\n",
      "\n",
      "Good morning, everyone,\n",
      "\n",
      "I hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developments and key information. Please find below the timeline for our upcoming meetings:\n",
      "\n",
      "- USERNAME - Meeting at TIME\n",
      "- USERNAME - Meeting at TIME\n",
      "- USERNAME - Meeting at TIME\n",
      "- USERNAME - Meeting at TIME\n",
      "- USERNAME \n",
      "112\n",
      "['[CLS]', '▁Subject', ':', '▁Group', '▁Messaging', '▁for', '▁Admissions', '▁Process', '▁Good', '▁morning', ',', '▁everyone', ',', '▁I', '▁hope', '▁this', '▁message', '▁finds', '▁you', '▁well', '.', '▁As', '▁we', '▁continue', '▁our', '▁admissions', '▁processes', ',', '▁I', '▁would', '▁like', '▁to', '▁update', '▁you', '▁on', '▁the', '▁latest', '▁developments', '▁and', '▁key', '▁information', '.', '▁Please', '▁find', '▁below', '▁the', '▁timeline', '▁for', '▁our', '▁upcoming', '▁meetings', ':', '▁-', '▁', 'wyn', 'q', 'vr', 'h', '053', '▁-', '▁Meeting', '▁at', '▁10:20', 'am', '▁-', '▁l', 'uka', '.', 'burg', '▁-', '▁Meeting', '▁at', '▁21', '▁-', '▁qa', 'hil', '.', 'witt', 'auer', '▁-', '▁Meeting', '▁at', '▁quarter', '▁past', '▁13', '▁-', '▁g', 'hola', 'm', 'hos', 's', 'ein', '.', 'ru', 'schke', '▁-', '▁Meeting', '▁at', '▁9', ':47', '▁PM', '▁-', '▁p', 'd', 'm', 'jr', 's', 'y', 'oz', '1', '460', '[SEP]']\n",
      "84\n",
      "['[CLS]', '▁Subject', ':', '▁Group', '▁Messaging', '▁for', '▁Admissions', '▁Process', '▁Good', '▁morning', ',', '▁everyone', ',', '▁I', '▁hope', '▁this', '▁message', '▁finds', '▁you', '▁well', '.', '▁As', '▁we', '▁continue', '▁our', '▁admissions', '▁processes', ',', '▁I', '▁would', '▁like', '▁to', '▁update', '▁you', '▁on', '▁the', '▁latest', '▁developments', '▁and', '▁key', '▁information', '.', '▁Please', '▁find', '▁below', '▁the', '▁timeline', '▁for', '▁our', '▁upcoming', '▁meetings', ':', '▁-', '▁USER', 'NAME', '▁-', '▁Meeting', '▁at', '▁TIME', '▁-', '▁USER', 'NAME', '▁-', '▁Meeting', '▁at', '▁TIME', '▁-', '▁USER', 'NAME', '▁-', '▁Meeting', '▁at', '▁TIME', '▁-', '▁USER', 'NAME', '▁-', '▁Meeting', '▁at', '▁TIME', '▁-', '▁USER', 'NAME', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = []\n",
    "from ast import literal_eval\n",
    "for item in train_dataset: \n",
    "    label_string = replace_labels(item['full_text'], literal_eval(item['span_labels']))\n",
    "    print(item['full_text'])\n",
    "    print(label_string)\n",
    "    tokenized_inputs = deberta_tokenizer(item[\"full_text\"], truncation=True, is_split_into_words=False)\n",
    "    label_tokenized_inputs = deberta_tokenizer(label_string, truncation=True, is_split_into_words=False)\n",
    "    print(len(tokenized_inputs['input_ids']))\n",
    "    print(deberta_tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids']))\n",
    "    print(len(label_tokenized_inputs['input_ids']))\n",
    "    print(deberta_tokenizer.convert_ids_to_tokens(label_tokenized_inputs['input_ids']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForTokenClassification, TrainingArguments, Trainer\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/deberta-v2-xlarge\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mall_labels\u001b[49m), id2label\u001b[38;5;241m=\u001b[39mid2label, label2id\u001b[38;5;241m=\u001b[39mlabel2id\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v2-xlarge\", num_labels=len(all_labels), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
