{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from aux import json_to_Dataset_ensemble\n",
    "import aux\n",
    "from tqdm import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1141 [00:00<?, ?it/s]/var/folders/05/8k53g1bs725dn8cs5310zydm0000gn/T/ipykernel_77993/667636356.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  stacked_tensors1 = torch.stack([torch.tensor(i) for i in output1])\n",
      "/var/folders/05/8k53g1bs725dn8cs5310zydm0000gn/T/ipykernel_77993/667636356.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  stacked_tensors2 = torch.stack([torch.tensor(i) for i in output2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([125, 47])\n",
      "torch.Size([85, 47])\n",
      "torch.Size([97, 47])\n",
      "torch.Size([77, 47])\n",
      "torch.Size([87, 47])\n",
      "torch.Size([89, 47])\n",
      "torch.Size([66, 47])\n",
      "torch.Size([54, 47])\n",
      "torch.Size([85, 47])\n",
      "torch.Size([68, 47])\n",
      "torch.Size([109, 47])\n",
      "torch.Size([86, 47])\n",
      "torch.Size([117, 47])\n",
      "torch.Size([75, 47])\n",
      "torch.Size([69, 47])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1141 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([85, 47])\n",
      "[125, 85, 98, 77, 87, 89, 66, 54, 85, 68, 109, 86, 117, 75, 69, 85]\n",
      "torch.Size([1374, 47])\n",
      "1375\n",
      "Error during batch processing: Expected input batch_size (1374) to match target batch_size (16).\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/05/8k53g1bs725dn8cs5310zydm0000gn/T/ipykernel_77993/1711267779.py\", line 145, in <module>\n",
      "    loss = criterion(output, targets)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py\", line 1185, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py\", line 3086, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: Expected input batch_size (1374) to match target batch_size (16).\n",
      "\n",
      "Epoch [1/1], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from aux import ensembler, json_to_Dataset_ensemble\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KingBert(nn.Module):\n",
    "    def __init__(self, distilbert_tuned, albert_tuned):\n",
    "        super().__init__()\n",
    "        self.distilbert = distilbert_tuned\n",
    "        self.albert = albert_tuned\n",
    "\n",
    "        for distilbert_param in self.distilbert.parameters():\n",
    "            distilbert_param.requires_grad = False\n",
    "\n",
    "        for albert_param in self.albert.parameters():\n",
    "            albert_param.requires_grad = False \n",
    "        \n",
    "        # Here we have an alpha for each label\n",
    "        self.alpha = nn.Parameter(0.5 * torch.ones(47), requires_grad=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, distilbert_input_ids, albert_input_ids, distil_attention_mask, alb_attention_mask, distilbert_word_ids, albert_word_ids):\n",
    "        distilbert_output = self.distilbert(input_ids=distilbert_input_ids, attention_mask=distil_attention_mask)\n",
    "        albert_output = self.albert(input_ids=albert_input_ids, attention_mask=alb_attention_mask)\n",
    "        distilbert_fixed, albert_fixed = aux.ensembler(distilbert_output['logits'].squeeze(), albert_output['logits'].squeeze(), distilbert_word_ids.squeeze(), albert_word_ids.squeeze())\n",
    "\n",
    "        distilbert_fixed = self.softmax(distilbert_fixed)\n",
    "        albert_fixed = self.softmax(albert_fixed)\n",
    "\n",
    "        final_output = distilbert_fixed * self.alpha + albert_fixed * (torch.ones(47) - self.alpha)\n",
    "\n",
    "        return self.softmax(final_output)\n",
    "\n",
    "# Load Huggingface dataset\n",
    "train_dataset = json_to_Dataset_ensemble('ensemble_train.json')\n",
    "\n",
    "# Load pre-trained models (distilbert and albert)\n",
    "distilbert_tuned = AutoModelForTokenClassification.from_pretrained('distilbert_finetuned')\n",
    "albert_tuned = AutoModelForTokenClassification.from_pretrained('albert_finetuned')\n",
    "\n",
    "kingbert_model = KingBert(distilbert_tuned, albert_tuned)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(kingbert_model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in tqdm(range(len(train_dataset)), desc=\"Steps in epoch\"):\n",
    "        try:\n",
    "            # Get the individual item from the dataset\n",
    "            item = train_dataset[i]\n",
    "            \n",
    "            distilbert_input_ids = torch.tensor(item['distilbert_inputids']).unsqueeze(0)\n",
    "            albert_input_ids = torch.tensor(item['albert_inputids']).unsqueeze(0)\n",
    "            distil_attention_mask = torch.tensor(item['distilbert_attention_masks']).unsqueeze(0)\n",
    "            alb_attention_mask = torch.tensor(item['albert_attention_masks']).unsqueeze(0)\n",
    "            distilbert_word_ids = torch.tensor([-100] + item['distilbert_wordids'][1:-1] + [-100]).unsqueeze(0)\n",
    "            albert_word_ids = torch.tensor([-100] + item['albert_wordids'][1:-1] + [-100]).unsqueeze(0)\n",
    "            targets = torch.tensor(item['spacy_labels']).unsqueeze(0)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = kingbert_model(distilbert_input_ids, albert_input_ids, distil_attention_mask, alb_attention_mask, distilbert_word_ids, albert_word_ids)\n",
    "            \n",
    "            ohe_targets = torch.zeros(output.shape[0], output.shape[1])\n",
    "            for i,j in enumerate(targets):\n",
    "                ohe_targets[i][j] = 1\n",
    "            # Compute loss\n",
    "            loss = criterion(output, ohe_targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    avg_loss = total_loss / len(train_dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "torch.save(kingbert_model.state_dict(), 'model_state.pth')\n",
    "\n",
    "print('Training complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
